{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dennistay1981/Resources/blob/main/Code%20and%20data%20in%20publications/Article%3A%20Fingerprints%20of%20EFL%20writing.%20An%20AI%20Deep%20Learning%20Approach/VAE_CLUSTER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df29323a",
      "metadata": {
        "id": "df29323a"
      },
      "outputs": [],
      "source": [
        "#Install and import libraries\n",
        "!pip install python-docx\n",
        "!pip install contractions\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import numpy as np\n",
        "import string\n",
        "import re\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from docx import Document\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "import contractions\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from sklearn.decomposition import PCA\n",
        "import seaborn as sns\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tqdm import tqdm\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d146cbb8",
      "metadata": {
        "id": "d146cbb8"
      },
      "outputs": [],
      "source": [
        "#Import texts. Ensure that all texts are in the directory defined as base_dir\n",
        "def extract_text_from_docx(file_path):\n",
        "    doc = Document(file_path)\n",
        "    full_text = []\n",
        "    for para in doc.paragraphs:\n",
        "        full_text.append(para.text)\n",
        "    return '\\n'.join(full_text)\n",
        "\n",
        "# Base directory (containing all .docx files)\n",
        "base_dir = r'C:\\your_directory'\n",
        "\n",
        "# Process all documents\n",
        "essays = []\n",
        "for file in os.listdir(base_dir):\n",
        "    if file.endswith('.docx') and not file.startswith('~$'):\n",
        "        try:\n",
        "            file_path = os.path.join(base_dir, file)\n",
        "            text = extract_text_from_docx(file_path)\n",
        "            essays.append({'filename': file, 'text': text})\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {file}: {e}\")\n",
        "\n",
        "# Create DataFrame\n",
        "essays_df = pd.DataFrame(essays)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3615e998",
      "metadata": {
        "id": "3615e998"
      },
      "source": [
        "\n",
        "Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81643696",
      "metadata": {
        "id": "81643696"
      },
      "outputs": [],
      "source": [
        "# Function for comprehensive text preprocessing\n",
        "def preprocess_text(text):\n",
        "\n",
        "    # handle possessives\n",
        "    text = re.sub(r\"'s\\b\", \"\", text)\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Expand contractions\n",
        "    text = contractions.fix(text)\n",
        "    # Directly remove apostrophes\n",
        "    text = text.replace(\"â€™\", \"\")\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    # Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    # Remove whitespace\n",
        "    text = text.strip()\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    # Remove tabs\n",
        "    text = re.sub(r'\\t', ' ', text)   # Remove actual tab\n",
        "    # Return the text\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15fe5d57",
      "metadata": {
        "id": "15fe5d57"
      },
      "outputs": [],
      "source": [
        "# Apply preprocessing to the text column\n",
        "essays_df['corrected_text'] = essays_df['text'].apply(preprocess_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a39e8e2",
      "metadata": {
        "id": "8a39e8e2"
      },
      "outputs": [],
      "source": [
        "# Word Tokenization\n",
        "essays_df['word_tokens'] = essays_df['corrected_text'].apply(lambda x:word_tokenize(x))\n",
        "essays_df['word_count'] = essays_df['word_tokens'].apply(lambda x: len(x))\n",
        "print('\\nUnique word count: ')\n",
        "print(essays_df['word_count'].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6200bfa",
      "metadata": {
        "id": "d6200bfa"
      },
      "outputs": [],
      "source": [
        "# Stop-word removal (skip punctuation check)\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def categorize_tokens(tokens):\n",
        "    filtered_tokens = []\n",
        "    stopwords_found = []\n",
        "\n",
        "    for token in tokens:\n",
        "        if token in stop_words and token not in stopwords_found:\n",
        "            stopwords_found.append(token)\n",
        "        elif token not in stop_words:\n",
        "            filtered_tokens.append(token)\n",
        "\n",
        "    return filtered_tokens, len(filtered_tokens), stopwords_found"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60523e5c",
      "metadata": {
        "id": "60523e5c"
      },
      "outputs": [],
      "source": [
        "# Apply to the word tokens column\n",
        "essays_df[['filtered_tokens', 'tokens_count', 'stopwords_found']] = pd.DataFrame(essays_df['word_tokens'].apply(categorize_tokens).tolist(), index = essays_df.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d45ff234",
      "metadata": {
        "id": "d45ff234"
      },
      "outputs": [],
      "source": [
        "# Lemmatization with POS tagging\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def get_wordnet_pos(word):\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\n",
        "        \"J\": wordnet.ADJ,\n",
        "        \"N\": wordnet.NOUN,\n",
        "        \"V\": wordnet.VERB,\n",
        "        \"R\": wordnet.ADV\n",
        "    }\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "# Apply lemmatization safely\n",
        "def lemmatize_token_list(token_list):\n",
        "    return [lemmatizer.lemmatize(str(token), get_wordnet_pos(str(token)))\n",
        "            for token in token_list if token]  # Added check for empty tokens\n",
        "\n",
        "essays_df['lemmatized_tokens'] = essays_df['filtered_tokens'].apply(lemmatize_token_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a23e96b",
      "metadata": {
        "id": "5a23e96b"
      },
      "source": [
        "SBERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79d4a413",
      "metadata": {
        "id": "79d4a413"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load model\n",
        "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Create embeddings\n",
        "sbert_vectors = sbert_model.encode(essays_df['text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d37e76d",
      "metadata": {
        "id": "1d37e76d"
      },
      "source": [
        "BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "306050be",
      "metadata": {
        "id": "306050be"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "def get_bert_embedding(text, max_length=512, max_chunks=5):\n",
        "    # Tokenize and truncate\n",
        "    tokens = tokenizer(text, padding='max_length', truncation=True,\n",
        "                       max_length=max_length, return_tensors='pt')\n",
        "\n",
        "    # Get BERT embeddings\n",
        "    with torch.no_grad():\n",
        "        outputs = bert(**tokens)\n",
        "\n",
        "    # Use CLS token embedding (sentence representation)\n",
        "    return outputs.pooler_output[0].numpy()\n",
        "\n",
        "# Process each essay\n",
        "embeddings = []\n",
        "\n",
        "for i, essay in enumerate(essays_df['text']):\n",
        "    # Tokenize the essay\n",
        "    encoding = tokenizer(\n",
        "        essay,\n",
        "        max_length=512,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    # Get BERT embeddings\n",
        "    with torch.no_grad():\n",
        "        outputs = bert(input_ids=encoding['input_ids'],\n",
        "                      attention_mask=encoding['attention_mask'])\n",
        "        # Use the [CLS] token as essay representation\n",
        "        cls_embedding = outputs.last_hidden_state[:, 0, :].numpy()\n",
        "\n",
        "    embeddings.append(cls_embedding[0])\n",
        "\n",
        "    if (i + 1) % 50 == 0:\n",
        "        print(f\"Processed {i + 1}/{len(essays_df)} essays\")\n",
        "\n",
        "bert_embedding = embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61951038",
      "metadata": {
        "id": "61951038"
      },
      "source": [
        "TRAIN VAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bff0285",
      "metadata": {
        "id": "1bff0285"
      },
      "outputs": [],
      "source": [
        "class VAE_LayerNorm(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=128, latent_dim=64, dropout_rate=0.2):\n",
        "        super(VAE_LayerNorm, self).__init__()\n",
        "\n",
        "        # Encoder layers using LayerNorm instead of BatchNorm\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim*2),\n",
        "            nn.LayerNorm(hidden_dim*2),  # LayerNorm instead of BatchNorm\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(dropout_rate),\n",
        "\n",
        "            nn.Linear(hidden_dim*2, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),    # LayerNorm instead of BatchNorm\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(dropout_rate),\n",
        "        )\n",
        "\n",
        "        # Mean and log variance layers for the latent space\n",
        "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
        "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
        "\n",
        "        # Decoder layers using LayerNorm instead of BatchNorm\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),    # LayerNorm instead of BatchNorm\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(dropout_rate),\n",
        "\n",
        "            nn.Linear(hidden_dim, hidden_dim*2),\n",
        "            nn.LayerNorm(hidden_dim*2),  # LayerNorm instead of BatchNorm\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(dropout_rate),\n",
        "\n",
        "            nn.Linear(hidden_dim*2, input_dim)\n",
        "        )\n",
        "\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def encode(self, x):\n",
        "        \"\"\"Encode input to latent distribution parameters\"\"\"\n",
        "        hidden = self.encoder(x)\n",
        "        mu = self.fc_mu(hidden)\n",
        "        logvar = self.fc_logvar(hidden)\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        \"\"\"Reparameterization trick to sample from latent space\"\"\"\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        z = mu + eps * std\n",
        "        return z\n",
        "\n",
        "    def decode(self, z):\n",
        "        \"\"\"Decode latent samples back to input space\"\"\"\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass through the VAE\"\"\"\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "\n",
        "    def get_latent(self, x):\n",
        "        \"\"\"Get the latent representation without sampling\"\"\"\n",
        "        mu, _ = self.encode(x)\n",
        "        return mu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f275cdd",
      "metadata": {
        "id": "7f275cdd"
      },
      "outputs": [],
      "source": [
        "def vae_loss_function(recon_x, x, mu, logvar, beta=1.0):\n",
        "    # Reconstruction loss (MSE)\n",
        "    MSE = F.mse_loss(recon_x, x, reduction='sum')\n",
        "\n",
        "    # KL divergence\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "\n",
        "    return MSE + beta * KLD, MSE, KLD\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78764c20",
      "metadata": {
        "id": "78764c20"
      },
      "outputs": [],
      "source": [
        "def train_vae_layernorm(vectors, model_name, batch_size=64, epochs=100, learning_rate=1e-4,\n",
        "                         hidden_dim=128, latent_dim=64, beta=1.0, min_batch_size=4):\n",
        "\n",
        "    # Convert to dense if sparse\n",
        "    if not isinstance(vectors, np.ndarray):\n",
        "        print(\"Converting sparse matrix to dense array...\")\n",
        "        vectors = vectors.toarray()\n",
        "\n",
        "    # Standardize the data\n",
        "    print(f\"Standardizing {vectors.shape[0]} vectors of dimension {vectors.shape[1]}...\")\n",
        "    scaler = StandardScaler()\n",
        "    vectors_scaled = scaler.fit_transform(vectors)\n",
        "\n",
        "    # Convert to torch tensors\n",
        "    tensor_x = torch.tensor(vectors_scaled, dtype=torch.float32)\n",
        "\n",
        "    # Create DataLoader\n",
        "    dataset = TensorDataset(tensor_x)\n",
        "\n",
        "    # Calculate appropriate batch size (make sure it's not too small)\n",
        "    effective_batch_size = min(batch_size, max(min_batch_size, len(dataset) // 10))\n",
        "    if effective_batch_size < batch_size:\n",
        "        print(f\"Warning: Adjusted batch size from {batch_size} to {effective_batch_size} to ensure enough batches\")\n",
        "\n",
        "    dataloader = DataLoader(dataset, batch_size=effective_batch_size, shuffle=True)\n",
        "\n",
        "    # Create VAE model\n",
        "    input_dim = vectors.shape[1]\n",
        "    print(f\"Creating VAE model with input_dim={input_dim}, hidden_dim={hidden_dim}, latent_dim={latent_dim}\")\n",
        "    vae = VAE_LayerNorm(input_dim=input_dim, hidden_dim=hidden_dim, latent_dim=latent_dim)\n",
        "\n",
        "    # Move to device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "    vae.to(device)\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.Adam(vae.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Training loop\n",
        "    train_losses = []\n",
        "    recon_losses = []\n",
        "    kl_losses = []\n",
        "\n",
        "    print(f\"Starting training for {model_name} ({epochs} epochs)...\")\n",
        "    try:\n",
        "        for epoch in range(epochs):\n",
        "            epoch_loss = 0\n",
        "            epoch_recon = 0\n",
        "            epoch_kl = 0\n",
        "\n",
        "            vae.train()\n",
        "            for batch_idx, (data,) in enumerate(dataloader):\n",
        "                data = data.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                recon_batch, mu, logvar = vae(data)\n",
        "                loss, recon, kl = vae_loss_function(recon_batch, data, mu, logvar, beta=beta)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "                epoch_recon += recon.item()\n",
        "                epoch_kl += kl.item()\n",
        "\n",
        "            # Calculate average losses\n",
        "            avg_loss = epoch_loss / len(dataloader.dataset)\n",
        "            avg_recon = epoch_recon / len(dataloader.dataset)\n",
        "            avg_kl = epoch_kl / len(dataloader.dataset)\n",
        "\n",
        "            train_losses.append(avg_loss)\n",
        "            recon_losses.append(avg_recon)\n",
        "            kl_losses.append(avg_kl)\n",
        "\n",
        "            if (epoch + 1) % 5 == 0 or epoch == 0:\n",
        "                print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}, Recon: {avg_recon:.4f}, KL: {avg_kl:.4f}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during training: {e}\")\n",
        "        print(\"Saving the model up to this point...\")\n",
        "\n",
        "    # Plot training curves\n",
        "    try:\n",
        "        plt.figure(figsize=(12, 4))\n",
        "\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(train_losses, label='Total Loss')\n",
        "        plt.title(f'VAE Training Loss - {model_name}')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(recon_losses, label='Reconstruction')\n",
        "        plt.plot(kl_losses, label='KL Divergence')\n",
        "        plt.title(f'Loss Components - {model_name}')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'vae_training_{model_name}.png')\n",
        "        plt.show()\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating plots: {e}\")\n",
        "\n",
        "    return vae, scaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c33d8447",
      "metadata": {
        "id": "c33d8447"
      },
      "outputs": [],
      "source": [
        "def get_latent_representations_layernorm(vae, vectors, scaler, batch_size=128):\n",
        "\n",
        "    # Convert to dense if sparse\n",
        "    if not isinstance(vectors, np.ndarray):\n",
        "        vectors = vectors.toarray()\n",
        "\n",
        "    # Standardize using the same scaler\n",
        "    vectors_scaled = scaler.transform(vectors)\n",
        "\n",
        "    # Convert to torch tensor\n",
        "    tensor_x = torch.tensor(vectors_scaled, dtype=torch.float32)\n",
        "    dataset = TensorDataset(tensor_x)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    vae.to(device)\n",
        "    vae.eval()\n",
        "\n",
        "    latent_vectors = []\n",
        "    with torch.no_grad():\n",
        "        for (data,) in dataloader:\n",
        "            data = data.to(device)\n",
        "            mu, _ = vae.encode(data)\n",
        "            latent_vectors.append(mu.cpu().numpy())\n",
        "\n",
        "    return np.vstack(latent_vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c792c67d",
      "metadata": {
        "id": "c792c67d"
      },
      "outputs": [],
      "source": [
        "def process_embedding_layernorm(name, vectors, hidden_dim=128, latent_dim=64, beta=1.0, epochs=50, batch_size=64):\n",
        "\n",
        "    print(f\"\\nTraining VAE for {name} embeddings...\")\n",
        "    vae, scaler = train_vae_layernorm(\n",
        "        vectors=vectors,\n",
        "        model_name=name,\n",
        "        hidden_dim=hidden_dim,\n",
        "        latent_dim=latent_dim,\n",
        "        beta=beta,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size\n",
        "    )\n",
        "\n",
        "    # Get latent representations\n",
        "    latent_vectors = get_latent_representations_layernorm(vae, vectors, scaler)\n",
        "\n",
        "    print(f\"Completed VAE training for {name}. Latent vectors shape: {latent_vectors.shape}\")\n",
        "\n",
        "    # Save the model and latent vectors\n",
        "    save_dir = f\"vae_{name}\"\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "\n",
        "    # Save VAE model\n",
        "    torch.save(vae.state_dict(), f\"{save_dir}/vae_model.pt\")\n",
        "\n",
        "    # Save scaler\n",
        "    with open(f\"{save_dir}/scaler.pkl\", 'wb') as f:\n",
        "        pickle.dump(scaler, f)\n",
        "\n",
        "    # Save latent vectors\n",
        "    np.save(f\"{save_dir}/latent_vectors.npy\", latent_vectors)\n",
        "\n",
        "    print(f\"Saved model and latent vectors to {save_dir}/\")\n",
        "\n",
        "    return latent_vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c02a9c51",
      "metadata": {
        "id": "c02a9c51"
      },
      "outputs": [],
      "source": [
        "# SBERT\n",
        "sbert_latent = process_embedding_layernorm(\"SBERT\", sbert_vectors, epochs=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5013e3c0",
      "metadata": {
        "id": "5013e3c0"
      },
      "outputs": [],
      "source": [
        "# BERT\n",
        "bert_latent = process_embedding_layernorm(\"Bert\", np.array(bert_embedding), epochs=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d099e673",
      "metadata": {
        "id": "d099e673"
      },
      "outputs": [],
      "source": [
        "# List of embedding approaches (replace with whatever you want to compare)\n",
        "approaches = [\"SBERT\", \"Bert\"]\n",
        "\n",
        "# Results dictionaries\n",
        "kmeans_results = {}\n",
        "gmm_results = {}\n",
        "\n",
        "# Count how many embedding approaches actually exist\n",
        "available_approaches = []\n",
        "for approach in approaches:\n",
        "    vectors_path = f\"vae_{approach}/latent_vectors.npy\"\n",
        "    if os.path.exists(vectors_path):\n",
        "        available_approaches.append(approach)\n",
        "\n",
        "# Calculate grid dimensions for plots\n",
        "n_approaches = len(available_approaches)\n",
        "n_cols = 2\n",
        "n_rows = (n_approaches + 1) // 2  # Ceiling division to ensure enough space\n",
        "\n",
        "# Set up the visualization figure\n",
        "plt.figure(figsize=(18, 4 * n_rows))\n",
        "\n",
        "for i, approach in enumerate(approaches):\n",
        "    # Path to the saved latent vectors\n",
        "    vectors_path = f\"vae_{approach}/latent_vectors.npy\"\n",
        "\n",
        "    # Skip if file doesn't exist\n",
        "    if not os.path.exists(vectors_path):\n",
        "        print(f\"No saved vectors found for {approach}\")\n",
        "        continue\n",
        "\n",
        "    # Load latent vectors\n",
        "    latent_vectors = np.load(vectors_path)\n",
        "    print(f\"Loaded {approach} latent vectors with shape {latent_vectors.shape}\")\n",
        "\n",
        "    # Apply PCA for visualization (reduce to 2D)\n",
        "    pca = PCA(n_components=2)\n",
        "    reduced_vectors = pca.fit_transform(latent_vectors)\n",
        "\n",
        "    # K-means Clustering\n",
        "    kmeans_scores = []\n",
        "    kmeans_best_score = -1\n",
        "    kmeans_best_k = 0\n",
        "    kmeans_best_labels = None\n",
        "\n",
        "    for k in range(2, 15):\n",
        "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "        clusters = kmeans.fit_predict(latent_vectors)\n",
        "        score = silhouette_score(latent_vectors, clusters)\n",
        "        kmeans_scores.append(score)\n",
        "\n",
        "        print(f\"{approach} K-means with {k} clusters: silhouette = {score:.4f}\")\n",
        "\n",
        "        if score > kmeans_best_score:\n",
        "            kmeans_best_score = score\n",
        "            kmeans_best_k = k\n",
        "            kmeans_best_labels = clusters\n",
        "\n",
        "    kmeans_results[approach] = {\n",
        "        \"best_k\": kmeans_best_k,\n",
        "        \"best_score\": kmeans_best_score,\n",
        "        \"all_scores\": kmeans_scores,\n",
        "        \"labels\": kmeans_best_labels\n",
        "    }\n",
        "\n",
        "    print(f\"Best K-means clustering for {approach}: {kmeans_best_k} clusters with score {kmeans_best_score:.4f}\")\n",
        "\n",
        "    # GMM Clustering\n",
        "    gmm_scores = []\n",
        "    gmm_best_score = -1\n",
        "    gmm_best_k = 0\n",
        "    gmm_best_labels = None\n",
        "\n",
        "    for k in range(2, 15):\n",
        "        gmm = GaussianMixture(n_components=k, random_state=42, n_init=10)\n",
        "        gmm.fit(latent_vectors)\n",
        "        clusters = gmm.predict(latent_vectors)\n",
        "        score = silhouette_score(latent_vectors, clusters)\n",
        "        gmm_scores.append(score)\n",
        "\n",
        "        print(f\"{approach} GMM with {k} clusters: silhouette = {score:.4f}\")\n",
        "\n",
        "        if score > gmm_best_score:\n",
        "            gmm_best_score = score\n",
        "            gmm_best_k = k\n",
        "            gmm_best_labels = clusters\n",
        "\n",
        "    gmm_results[approach] = {\n",
        "        \"best_k\": gmm_best_k,\n",
        "        \"best_score\": gmm_best_score,\n",
        "        \"all_scores\": gmm_scores,\n",
        "        \"labels\": gmm_best_labels\n",
        "    }\n",
        "\n",
        "    print(f\"Best GMM clustering for {approach}: {gmm_best_k} clusters with score {gmm_best_score:.4f}\")\n",
        "    print()\n",
        "\n",
        "    # ----------------- Visualization -----------------\n",
        "    # Plot 1: K-means vs GMM silhouette scores\n",
        "    plt.subplot(n_rows, n_cols, i+1)\n",
        "    x = list(range(2, 15))\n",
        "    plt.plot(x, kmeans_scores, 'o-', label='K-means')\n",
        "    plt.plot(x, gmm_scores, 's-', label='GMM')\n",
        "    plt.xlabel('Number of clusters')\n",
        "    plt.ylabel('Silhouette Score')\n",
        "    plt.title(f'{approach} Clustering Performance')\n",
        "    plt.legend()\n",
        "    plt.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e1e6e83",
      "metadata": {
        "id": "7e1e6e83"
      },
      "outputs": [],
      "source": [
        "# Generate scatter plots of the best clustering for each approach\n",
        "for approach in approaches:\n",
        "    if approach not in kmeans_results:\n",
        "        continue\n",
        "\n",
        "    # Path to the saved latent vectors and load them\n",
        "    vectors_path = f\"vae_{approach}/latent_vectors.npy\"\n",
        "    latent_vectors = np.load(vectors_path)\n",
        "\n",
        "    # Apply PCA for visualization\n",
        "    pca = PCA(n_components=2)\n",
        "    reduced_vectors = pca.fit_transform(latent_vectors)\n",
        "\n",
        "    # Create a figure with two subplots side by side\n",
        "    plt.figure(figsize=(16, 7))\n",
        "\n",
        "    # K-means visualization\n",
        "    plt.subplot(1, 2, 1)\n",
        "    kmeans_labels = kmeans_results[approach][\"labels\"]\n",
        "    plt.scatter(reduced_vectors[:, 0], reduced_vectors[:, 1], c=kmeans_labels, cmap='tab10', alpha=0.7)\n",
        "    plt.title(f'{approach} - K-means (k={kmeans_results[approach][\"best_k\"]}, score={kmeans_results[approach][\"best_score\"]:.4f})')\n",
        "    plt.xlabel('PCA Component 1')\n",
        "    plt.ylabel('PCA Component 2')\n",
        "    plt.colorbar(label='Cluster')\n",
        "\n",
        "    # GMM visualization\n",
        "    plt.subplot(1, 2, 2)\n",
        "    gmm_labels = gmm_results[approach][\"labels\"]\n",
        "    plt.scatter(reduced_vectors[:, 0], reduced_vectors[:, 1], c=gmm_labels, cmap='tab10', alpha=0.7)\n",
        "    plt.title(f'{approach} - GMM (k={gmm_results[approach][\"best_k\"]}, score={gmm_results[approach][\"best_score\"]:.4f})')\n",
        "    plt.xlabel('PCA Component 1')\n",
        "    plt.ylabel('PCA Component 2')\n",
        "    plt.colorbar(label='Cluster')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}