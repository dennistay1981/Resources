{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dennistay1981/Resources/blob/main/Quant_schemes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWCPiL3_vxzk"
      },
      "source": [
        "Importing libraries and data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Tpokj6hmvkSM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pd.set_option('display.max_rows',None)\n",
        "pd.set_option('display.max_columns',None)\n",
        "pd.set_option('display.width', 1000)\n",
        "\n",
        "from pylab import rcParams\n",
        "rcParams['figure.figsize']=12,6\n",
        "rcParams['figure.dpi']=300"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nd7MpEpW2cJE"
      },
      "source": [
        "Importing text data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "mQIO1L0i0B62",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "data=pd.read_csv('Tinder_tfidf(2D).csv')\n",
        "data=pd.read_csv('Tinder.csv')\n",
        "\n",
        "#cleaning\n",
        "import re\n",
        "from nltk.stem import *\n",
        "p_stemmer = PorterStemmer()\n",
        "\n",
        "# Remove punctuation, special characters\n",
        "data['special_removed']=data['Post'].map(lambda x: re.sub(r'\\W', ' ', x))\n",
        "# Remove all single characters (e.g. s left behind after deleting aposthrophe)\n",
        "data['singlechar_removed']=data['special_removed'].map(lambda x: re.sub(r'\\s+[a-zA-Z]\\s+', ' ', x))\n",
        "# Substitute multiple spaces with single space (after removing single characters, double spaces are created)\n",
        "data['singlechar_removed2']=data['singlechar_removed'].map(lambda x: re.sub(r'\\s+', ' ', x, flags=re.I))\n",
        "# Remove prefixed 'b' (if text string is in bytes format, a character b is appended with the string. This removes it)\n",
        "data['b_removed']=data['singlechar_removed2'].map(lambda x: re.sub(r'^b\\s+', ' ', x, flags=re.I))\n",
        "# Convert the titles to lowercase\n",
        "data['lower_case'] = data['b_removed'].map(lambda x: x.lower())\n",
        "# Remove numbers (but not numbers within words)\n",
        "data['num_removed'] = data['lower_case'].map(lambda x: re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", \" \", x))\n",
        "# Stemming to remove morphological affixes from words, leaving only the word stem\n",
        "data['stemmed'] = data['num_removed'].map(lambda x: p_stemmer.stem(x))\n",
        "# Finally, create final cleaned column as 'processed'\n",
        "data['processed']=data['stemmed']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pWz7V1L6RP5"
      },
      "source": [
        "TF-IDF Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1EVjQmQ26Xln"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "#apply tfidf vectorizer\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1,1))  #process up to n-grams (contiguous sequence of n words)\n",
        "vectorizer.fit_transform(data['processed'])\n",
        "\n",
        "#see the list of words/features\n",
        "vectorizer.get_feature_names_out()\n",
        "\n",
        "#get document-term matrix. This is a 'dense matrix' because every element (including the many 0) is stored\n",
        "matrix=(vectorizer.fit_transform(data['processed']).toarray())\n",
        "\n",
        "#x documents, y unique words/features\n",
        "matrix.shape\n",
        "\n",
        "#convert matrix to dataframe, with each feature and its corresponding tfidf score\n",
        "df=pd.DataFrame(matrix, columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "#convert to csv if needed\n",
        "df.to_csv('df.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result_df = pd.concat([data, df], axis=1)\n",
        "\n",
        "\n",
        "result_df.to_csv('IMDB tfidf.csv')"
      ],
      "metadata": {
        "id": "KvHVlWGe-krg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ol_fvk5afzy"
      },
      "source": [
        "Visualizing outcomes by reducing to 2D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XFAC2wwaifI"
      },
      "outputs": [],
      "source": [
        "#PCA: reduce matrix to 2D if needed\n",
        "from sklearn.decomposition import PCA as sklearnPCA\n",
        "pca = sklearnPCA(n_components=2)\n",
        "pca.fit_transform(matrix)\n",
        "\n",
        "#view the linear combinations\n",
        "pca.components_\n",
        "\n",
        "#attach reduced 2D back to dataframe for future use\n",
        "data[['Dim1','Dim2']]=pca.fit_transform(matrix)\n",
        "\n",
        "sns.scatterplot(data,x='Dim1',y='Dim2', hue='Gender')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mPLW2Ym6hDu"
      },
      "source": [
        "WORD EMBEDDING with large pre-trained models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "CjbIHUqD6jYt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49001b75-5062-47fd-9032-8ac6ce8003b4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100, 300)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "#Install and import GENSIM\n",
        "!pip install --upgrade gensim\n",
        "import gensim.downloader as api\n",
        "\n",
        "#See list of available pre-trained models. Larger ones take longer to download.\n",
        "print(api.info()['models'].keys())\n",
        "\n",
        "# Load Google News model (300 dimensions)\n",
        "model = api.load(\"word2vec-google-news-300\")\n",
        "\n",
        "# Load glove-wiki-gigaword-50 (50 dimensions) (https://nlp.stanford.edu/projects/glove/)\n",
        "model = api.load(\"glove-wiki-gigaword-50\")\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Demonstrating word embedding features\n",
        "\"\"\"\n",
        "#displaying the vector for a certain word\n",
        "model['dog']\n",
        "model['not_a_word']\n",
        "\n",
        "#vector algebra\n",
        "#finding most similar words by specifying relations\n",
        "model.most_similar(positive=['woman', 'king'], negative=['male'])\n",
        "model.doesnt_match(\"breakfast cereal dinner lunch\".split())\n",
        "#calculating similarity index between word pairs\n",
        "model.similarity('woman', 'man')\n",
        "model.similarity('woman', 'literature')\n",
        "model.similarity('man', 'literature')\n",
        "model.similarity('woman', 'engineer')\n",
        "model.similarity('man', 'engineer')\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Derive word embeddings for our data\n",
        "\"\"\"\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "text_column = data['processed']\n",
        "# Convert the text to a list of sentences\n",
        "text_data = []\n",
        "for text in text_column:\n",
        "    sentence_list = nltk.sent_tokenize(text)\n",
        "    text_data.extend(sentence_list)\n",
        "# Preprocess the text data\n",
        "preprocessed_data = []\n",
        "for sentence in text_data:\n",
        "    preprocessed_sentence = [word.lower() for word in sentence.split() if word.isalpha()]\n",
        "    preprocessed_data.append(preprocessed_sentence)\n",
        "\n",
        "\n",
        "# Derive embeddings\n",
        "embedding_data = []\n",
        "for sentence in preprocessed_data:\n",
        "    sentence_embedding = [model.get_vector(word) for word in sentence if word in model.key_to_index]\n",
        "    if sentence_embedding:\n",
        "        embedding_data.append(sum(sentence_embedding) / len(sentence_embedding))\n",
        "    else:\n",
        "        embedding_data.append(None)\n",
        "\n",
        "#shape of embedding data (no. of sentences x 50 or 300 dimensions)\n",
        "np.array(embedding_data).shape\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Convert embeddings to a dataframe.\n",
        "Each row of the DataFrame corresponds to a sentence in the preprocessed data, and each column corresponds to a dimension of the Word2Vec embeddings.\n",
        "\"\"\"\n",
        "#Automatically name columns in sequence\n",
        "embedding = pd.DataFrame(embedding_data, columns=['Dim{}'.format(i) for i in range(1, np.array(embedding_data).shape[1]+ 1)])\n",
        "\n",
        "\n",
        "#reduce embedding to 2D with PCA, if needed\n",
        "from sklearn.decomposition import PCA as sklearnPCA\n",
        "pca = sklearnPCA(n_components=2)\n",
        "pca.fit_transform(embedding)\n",
        "\n",
        "\n",
        "#attach reduced 2D back to dataframe, for future use\n",
        "data3=pd.read_csv('Lecture9.csv')\n",
        "data3[['Dim1','Dim2']]=pca.fit_transform(embedding)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensemble model fitting"
      ],
      "metadata": {
        "id": "PqqA6OnbGunK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x=data[['Dim1','Dim2']]\n",
        "y=data['Gender']\n",
        "\n",
        "\n",
        "# Perform train-test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state=42, stratify=y)\n",
        "\n",
        "\n",
        "# We will use these three classifiers\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Instantiate them.\n",
        "# Classifiers have optimal parameters that should also be independently determined, to optimize the ensemble.\n",
        "# But we are skipping this step.\n",
        "knn = KNeighborsClassifier()\n",
        "lr = LogisticRegression()\n",
        "svc = SVC()\n",
        "\n",
        "# Decision trees and Naive bayes are another two common classifiers. We leave them out for now\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "dt = DecisionTreeClassifier()\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "nb = MultinomialNB()\n",
        "\n",
        "\n",
        "# Define our list of three classifiers.\n",
        "classifiers = [('K Nearest Neighbours',knn), ('Logistic Regression',lr), ('SVC',svc)]\n",
        "\n",
        "# Iterate over the pre-defined list of classifiers, and evaluate predictions\n",
        "for clf_name, clf in classifiers:\n",
        "    clf.fit(x_train, y_train)\n",
        "    print(clf_name,'Train accuracy:', clf.score(x_train,y_train), 'Test accuracy:', clf.score(x_test, y_test))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Use a VOTING CLASSIFIER to determine final result\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "# Instantiate voting classifier\n",
        "vc = VotingClassifier(estimators=classifiers)\n",
        "vc.fit(x_train, y_train)\n",
        "\n",
        "print('Voting Classifier train accuracy:', vc.score(x_train,y_train), 'Test accuracy:', vc.score(x_test,y_test))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Confusion matrix\n",
        "cnf_matrix = metrics.confusion_matrix(y, vc.predict(x))\n",
        "\n",
        "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"Blues\", yticklabels=labels, xticklabels=labels, annot_kws={\"size\": 25})\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n",
        "\n",
        "# Classification report\n",
        "print(metrics.classification_report(y, vc.predict(x)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvLbxBHFGrLu",
        "outputId": "107666d3-d57e-483a-a213-4bd2d6e2f798"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Voting Classifier train accuracy: 0.5625 Test accuracy: 0.55\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}